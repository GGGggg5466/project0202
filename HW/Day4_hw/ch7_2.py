import os
import json
import time
from typing import TypedDict, Literal

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END


# =============================
# 0) ENV / Models
# =============================
load_dotenv()

# æ…¢ä½†å¼·ï¼ˆExpertï¼‰ï¼šè€å¸«å¸¸ç”¨ ws-02 + gemma
EXPERT_BASE_URL = os.getenv("EXPERT_BASE_URL", os.getenv("BASE_URL", "https://ws-02.wade0426.me/v1"))
EXPERT_API_KEY = os.getenv("EXPERT_API_KEY", os.getenv("API_KEY", ""))
EXPERT_MODEL = os.getenv("EXPERT_MODEL", os.getenv("MODEL", "google/gemma-3-27b-it"))

# å¿«é€Ÿé€šé“ï¼ˆFastï¼‰ï¼šä½ è‡ªå·±çš„ ws-05 + qwen (æˆ–ä»»ä½•ä½ å¯ç”¨çš„å¿«é€Ÿæ¨¡å‹)
FAST_BASE_URL = os.getenv("FAST_BASE_URL", "https://ws-05.huannago.com/v1")
FAST_API_KEY = os.getenv("FAST_API_KEY", os.getenv("API_KEY", ""))
FAST_MODEL = os.getenv("FAST_MODEL", "Qwen3-VL-8B-Instruct-BF16.gguf")

# LLM clients
llm = ChatOpenAI(
    base_url=EXPERT_BASE_URL,
    api_key=EXPERT_API_KEY,
    model=EXPERT_MODEL,
    temperature=0.7,
)

fast_llm = ChatOpenAI(
    base_url=FAST_BASE_URL if FAST_BASE_URL else EXPERT_BASE_URL,
    api_key=FAST_API_KEY if FAST_API_KEY else EXPERT_API_KEY,
    model=FAST_MODEL if FAST_BASE_URL else EXPERT_MODEL,  # è‹¥æ²’å¡« fastï¼Œå°±é€€å› expert
    temperature=0,
)

CACHE_FILE = "qa_cache.json"


# =============================
# 1) Cache helpers
# =============================
def get_clean_key(text: str) -> str:
    # å»ç©ºç™½ã€å»å…¨å½¢æ¨™é»ï¼ˆç°¡å–®ç‰ˆï¼‰â€”è·ŸæŠ•å½±ç‰‡ä¸€è‡´å°±å¥½
    return (
        text.strip()
        .replace(" ", "")
        .replace("ï¼Ÿ", "?")
        .replace("ï¼", "!")
        .replace("ï¼Œ", ",")
        .replace("ã€‚", ".")
    )


def load_cache() -> dict:
    # è‹¥æª”æ¡ˆä¸å­˜åœ¨ï¼šå»ºç«‹é è¨­å¿«å–ï¼ˆæŠ•å½±ç‰‡å¸¸é€™æ¨£åšç¤ºç¯„ Cache Hitï¼‰
    if not os.path.exists(CACHE_FILE):
        default_data = {
            get_clean_key("LangGraphæ˜¯ä»€éº¼ï¼Ÿ"): "LangGraph æ˜¯ä¸€å€‹ç”¨ã€Œåœ–ï¼ˆGraphï¼‰ã€ä¾†ç·¨æ’ LLM workflow çš„æ¡†æ¶ï¼Œæ”¯æ´åˆ†æ”¯ã€è¿´åœˆã€ç‹€æ…‹ç®¡ç†èˆ‡å®¹éŒ¯ï¼Œé©åˆåšå¯æ§çš„ agent ç³»çµ±ã€‚",
            get_clean_key("ä½ çš„åå­—ï¼Ÿ"): "æˆ‘æ˜¯é€™å€‹ QA Chat çš„ AI åŠ©æ‰‹ï½",
        }
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(default_data, f, ensure_ascii=False, indent=4)
        return default_data

    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        return {}


def save_cache(new_data: dict) -> None:
    current_data = {}
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                current_data = json.load(f)
        except Exception:
            current_data = {}

    current_data.update(new_data)

    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(current_data, f, ensure_ascii=False, indent=4)


# =============================
# 2) State
# =============================
class State(TypedDict):
    question: str
    answer: str
    source: str  # CACHE / FAST_TRACK_API / LLM_EXPERT


# =============================
# 3) Nodes
# =============================
def check_cache_node(state: State) -> State:
    print(f"\n[ç³»çµ±] æ”¶åˆ°å•é¡Œï¼š{state['question']}")
    cache_data = load_cache()
    clean_query = get_clean_key(state["question"])

    if clean_query in cache_data:
        print("--- å‘½ä¸­å¿«å– (Cache Hit) ---")
        return {
            "answer": cache_data[clean_query],
            "source": "CACHE",
        }

    print("--- å¿«å–æœªå‘½ä¸­ (Cache Miss) ---")
    return {"answer": ""}


def fast_reply_node(state: State) -> State:
    print("--- é€²å…¥å¿«é€Ÿé€šé“ (Fast Track API) ---")
    resp = fast_llm.invoke([HumanMessage(content=state["question"])])
    return {"answer": resp.content.strip(), "source": "FAST_TRACK_API"}


def expert_node(state: State) -> State:
    print("--- é€²å…¥å°ˆå®¶æ¨¡å¼ (LLM Expert) ---")
    prompt = f"è«‹ç”¨ç¹é«”ä¸­æ–‡ï¼Œæ¸…æ¥šã€æœ‰æ¢ç†åœ°å›ç­”ä¸‹åˆ—å•é¡Œï¼š\n{state['question']}\n"
    # ç”¨ stream å°å‡ºï¼ˆæŠ•å½±ç‰‡æ•ˆæœï¼‰
    chunks = llm.stream([HumanMessage(content=prompt)])

    full_answer = ""
    print("ğŸ¤– AI æ­£åœ¨æ€è€ƒä¸¦è¼¸å‡ºï¼š", end="", flush=True)
    for chunk in chunks:
        content = getattr(chunk, "content", "")
        if content:
            print(content, end="", flush=True)
            full_answer += content
    print("\n")  # æ›è¡Œ

    clean_key = get_clean_key(state["question"])
    save_cache({clean_key: full_answer})
    print(f"--- [ç³»çµ±] å·²å°‡å®Œæ•´å›ç­”å¯«å…¥ {CACHE_FILE} ---")

    return {"answer": full_answer, "source": "LLM_EXPERT"}


# =============================
# 4) Router
# =============================
def master_router(state: State) -> Literal["end", "fast", "expert"]:
    # å¦‚æœ check_cache å·²ç¶“æœ‰ answerï¼Œå°±çµæŸ
    if state.get("answer"):
        return "end"

    q = state["question"]
    # æŠ•å½±ç‰‡ç¤ºç¯„ï¼šæ‹›å‘¼èªèµ° Fast Track
    if any(word in q for word in ["ä½ å¥½", "å—¨", "æ—©å®‰", "å“ˆå›‰"]):
        return "fast"
    return "expert"


# =============================
# 5) Build Graph
# =============================
def build_app():
    workflow = StateGraph(State)

    workflow.add_node("check_cache", check_cache_node)
    workflow.add_node("fast_bot", fast_reply_node)
    workflow.add_node("expert_bot", expert_node)

    workflow.set_entry_point("check_cache")

    workflow.add_conditional_edges(
        "check_cache",
        master_router,
        {
            "end": END,
            "fast": "fast_bot",
            "expert": "expert_bot",
        },
    )

    workflow.add_edge("fast_bot", END)
    workflow.add_edge("expert_bot", END)

    app = workflow.compile()
    print(app.get_graph().draw_ascii())
    return app


# =============================
# 6) CLI
# =============================
if __name__ == "__main__":
    print(f"å¿«å–æª”æ¡ˆè·¯å¾‘ï¼š{os.path.abspath(CACHE_FILE)}")
    print("æç¤ºï¼šè¼¸å…¥æ‹›å‘¼èªï¼ˆä½ å¥½/å—¨/æ—©å®‰/å“ˆå›‰ï¼‰èµ° Fast Trackï¼›ä¸€èˆ¬å•é¡Œèµ° Expertï¼›å‘½ä¸­ cache ç›´æ¥å›è¦†ã€‚")

    app = build_app()

    while True:
        user_input = input("\nè«‹è¼¸å…¥å•é¡Œï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰: ").strip()
        if user_input.lower() == "q":
            break

        inputs: State = {"question": user_input, "answer": "", "source": ""}

        start_time = time.time()
        try:
            result = app.invoke(inputs)
        except Exception as e:
            print(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            continue
        end_time = time.time()

        print("=" * 30)
        print(f"ä¾†æº: [{result.get('source', '')}]")
        print(f"è€—æ™‚: {end_time - start_time:.4f} ç§’")

        # Expert å·²ç¶“åœ¨ stream å°éä¸€æ¬¡ï¼Œé€™è£¡ç…§æŠ•å½±ç‰‡é‚è¼¯å¯é¸æ“‡ä¸é‡å°
        if result.get("source") != "LLM_EXPERT":
            print(f"å›ç­”:\n{result.get('answer','')}")
        else:
            print("(å›ç­”å·²åœ¨ä¸Šæ–¹ streaming è¼¸å‡ºå®Œæˆ)")
