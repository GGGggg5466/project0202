import os
import json
import time
from typing import TypedDict, List, Dict, Any, Literal, Optional
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END

# =========================
# 0) å¼•ç”¨å­¸é•·çš„æª”æ¡ˆï¼ˆä¸ä¿®æ”¹ï¼‰
# =========================
# ä½ çš„è³‡æ–™å¤¾è‹¥æ˜¯ï¼š
# Day4_hw/
#   homework.py   (æœ¬æª”)
#   search_searxng.py
#   vlm_read_website.py
#
# ç›´æ¥ import å°±æœƒæŠ“åˆ°åŒè³‡æ–™å¤¾çš„æ¨¡çµ„
from search_searxng import search_searxng
from vlm_read_website import vlm_read_website


# =========================
# 1) ç’°å¢ƒè®Šæ•¸ / è¨­å®š
# =========================
load_dotenv()

BASE_URL = os.getenv("BASE_URL", "https://ws-05.huannago.com/v1")
API_KEY = os.getenv("API_KEY", "")
MODEL = os.getenv("MODEL", "Qwen3-VL-8B-Instruct-BF16.gguf")

CACHE_FILE = os.getenv("CACHE_FILE", "verify_cache.json")
ENABLE_VLM_READ = os.getenv("ENABLE_VLM_READ", "False").lower() == "true"

print(f"CACHE_FILE: {os.path.abspath(CACHE_FILE)}")
print(f"ENABLE_VLM_READ: {ENABLE_VLM_READ}")

llm = ChatOpenAI(
    base_url=BASE_URL,
    api_key=API_KEY,
    model=MODEL,
    temperature=0.2,
)


# =========================
# 2) Cache utilities
# =========================
def get_clean_key(text: str) -> str:
    """è®“ cache key æ›´ç©©ï¼šå»é ­å°¾ç©ºç™½ã€çµ±ä¸€å…¨å½¢/åŠå½¢ç©ºç™½ã€ç§»é™¤å¤šé¤˜æ›è¡Œ"""
    return " ".join(text.strip().split())

def load_cache() -> Dict[str, Any]:
    if not os.path.exists(CACHE_FILE):
        return {}
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def save_cache(key: str, value: Dict[str, Any]) -> None:
    data = load_cache()
    data[key] = value
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# =========================
# 3) State å®šç¾©
# =========================
class State(TypedDict):
    question: str
    answer: str
    cache_hit: bool

    # knowledge_baseï¼šç´¯ç©è­‰æ“š/è³‡æ–™ï¼ˆå¯è¢« planner åˆ¤æ–·æ˜¯å¦è¶³å¤ ï¼‰
    kb: List[Dict[str, Any]]

    # search related
    search_query: str
    loop: int

    # æœ€çµ‚è¼¸å‡ºé™„å¸¶è³‡è¨Š
    evidence_summary: str


# =========================
# 4) Nodes
# =========================
def check_cache_node(state: State) -> State:
    q = get_clean_key(state["question"])
    cache = load_cache()

    print("\n--- [check_cache] æª¢æŸ¥å¿«å– ---")
    if q in cache:
        print("âœ… Cache Hit")
        state["cache_hit"] = True
        state["answer"] = cache[q].get("answer", "")
        state["evidence_summary"] = cache[q].get("evidence_summary", "")
        # kb ä¹Ÿå¯ä»¥å¸¶å›ä¾†ï¼ˆå¯é¸ï¼‰
        state["kb"] = cache[q].get("kb", [])
    else:
        print("âŒ Cache Miss")
        state["cache_hit"] = False
    return state


def planner_node(state: State) -> State:
    """
    æ ¸å¿ƒï¼šç”¨ knowledge_base åˆ¤æ–·ã€Œå¤ ä¸å¤ å›ç­”ã€
    - å¤ ï¼šèµ° final_answer
    - ä¸å¤ ï¼šèµ° query_genï¼ˆç”¢ç”Ÿé—œéµå­—å†å» search_toolï¼‰
    """
    print("\n--- [planner] è©•ä¼° knowledge_base æ˜¯å¦è¶³å¤  ---")

    q = state["question"]
    kb = state.get("kb", [])
    loop = state.get("loop", 0)

    # safetyï¼šé¿å…ç„¡é™å¾ªç’°
    if loop >= 3:
        print("âš ï¸ loop >= 3ï¼Œå¼·åˆ¶é€² final_answerï¼ˆç”¨ç¾æœ‰è³‡æ–™æ•´ç†å‡ºå¯å›ç­”çš„ç‰ˆæœ¬ï¼‰")
        return state

    # å¦‚æœ kb ç©ºï¼Œç›´æ¥åˆ¤å®šä¸å¤  -> å» query_gen
    if not kb:
        print("kb ç›®å‰æ˜¯ç©ºçš„ â†’ ä¸è¶³ï¼Œæº–å‚™å» query_gen")
        return state

    # ç”¨ LLM åˆ¤æ–·ã€Œè³‡æ–™æ˜¯å¦è¶³å¤ ã€
    kb_text = "\n".join(
        [f"- title: {x.get('title','')}\n  url: {x.get('url','')}\n  snippet: {x.get('snippet','')}"
         for x in kb[:8]]
    )

    prompt = f"""
ä½ æ˜¯ä¸€å€‹åš´è¬¹çš„æŸ¥è­‰åŠ©ç†ï¼Œè«‹åˆ¤æ–·ç›®å‰ knowledge_base æ˜¯å¦è¶³å¤ å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚

ã€å•é¡Œã€‘
{q}

ã€knowledge_base æ‘˜è¦ã€‘
{kb_text}

è«‹åªè¼¸å‡º JSONï¼ˆä¸è¦å¤šé¤˜æ–‡å­—ï¼‰ï¼š
{{
  "enough": true/false,
  "why": "ä¸€å¥è©±ç†ç”±",
  "next": "FINAL" æˆ– "SEARCH"
}}
"""
    resp = llm.invoke([HumanMessage(content=prompt)]).content.strip()

    # è§£æ JSONï¼ˆå®¹éŒ¯ï¼‰
    enough = False
    try:
        j = json.loads(resp)
        enough = bool(j.get("enough", False))
        why = j.get("why", "")
        nxt = j.get("next", "SEARCH")
    except Exception:
        why = "LLM å›å‚³é JSONï¼Œä¿å®ˆè¦–ç‚ºä¸è¶³"
        nxt = "SEARCH"

    print(f"planner åˆ¤æ–·ï¼šenough={enough} / next={nxt} / why={why}")

    return state


def query_gen_node(state: State) -> State:
    """
    ç”Ÿæˆé—œéµå­—ï¼ˆæˆ–æœå°‹ queryï¼‰ï¼Œäº¤çµ¦ search_tool ä½¿ç”¨ã€‚
    """
    print("\n--- [query_gen] ç”Ÿæˆæœå°‹é—œéµå­— ---")

    q = state["question"]
    prompt = f"""
ä½ æ˜¯æœå°‹é—œéµå­—ç”Ÿæˆå™¨ã€‚è«‹é‡å°å•é¡Œç”¢ç”Ÿ 1 æ¢æœ€é©åˆæ‹¿å» searXNG æœå°‹çš„æŸ¥è­‰é—œéµå­—/æŸ¥è©¢å¥ã€‚
è¦æ±‚ï¼š
- ä¸è¦å¤ªé•·ï¼ˆ20~60å­—ï¼‰
- å„ªå…ˆä½¿ç”¨å¯æŸ¥è­‰çš„å®˜æ–¹é—œéµå­—ï¼ˆæ©Ÿé—œ/çµ±è¨ˆ/å…¬å‘Š/å ±å‘Šï¼‰
- åªè¼¸å‡ºä¸€è¡Œ queryï¼ˆä¸è¦è§£é‡‹ï¼‰

å•é¡Œï¼š{q}
"""
    search_query = llm.invoke([HumanMessage(content=prompt)]).content.strip()
    state["search_query"] = search_query
    print(f"search_query = {search_query}")
    return state


def search_tool_node(state: State) -> State:
    """
    ç”¨å­¸é•·çš„ search_searxng() å»æŠ“è³‡æ–™ï¼Œå¯«å…¥ knowledge_baseã€‚
    æ³¨æ„ï¼šæ­¤ node ä¸æœƒç›´æ¥èµ° final/endï¼Œæœƒå›åˆ° planner å†è©•ä¼°ã€‚
    """
    print("\n--- [search_tool] searXNG æœå°‹ ---")

    query = state.get("search_query", "").strip()
    if not query:
        # æ²’ query å°±å›å»ï¼ˆè®“ planner å†æ±ºç­–æˆ–æœ€å¾Œå…œåº•ï¼‰
        print("âš ï¸ search_query ç©ºçš„ï¼Œç•¥éæœå°‹")
        return state

    results = search_searxng(query=query, limit=5)

    kb = state.get("kb", [])
    added = 0
    for r in results or []:
        item = {
            "title": r.get("title", ""),
            "url": r.get("url", r.get("link", "")),
            "snippet": r.get("snippet", r.get("content", "")),
            "query": query,
            "ts": time.time(),
        }
        kb.append(item)
        added += 1

    state["kb"] = kb
    state["loop"] = state.get("loop", 0) + 1
    print(f"å·²åŠ å…¥ kbï¼š{added} ç­†ï¼ˆloop={state['loop']}ï¼‰")

    return state


def final_answer_node(state: State) -> State:
    """
    æ•´ç†æœ€çµ‚ç­”æ¡ˆï¼š
    - è‹¥ cache_hit=Trueï¼šanswer å·²æœ‰ï¼ˆå¯é¸æ“‡å†åŒ…è£ï¼‰
    - å¦å‰‡ç”¨ kb ç”¢ç”Ÿã€Œçµè«– / è­‰æ“šæ‘˜è¦ / é™åˆ¶ã€
    ä¸¦å¯«å…¥ cacheã€‚
    """
    print("\n--- [final_answer] è¼¸å‡ºæœ€çµ‚ç­”æ¡ˆ ---")

    q = state["question"]
    kb = state.get("kb", [])

    # è‹¥æ˜¯ Cache Hit ç›´æ¥å›å‚³ï¼ˆä»å¯è£œä¸€è¡Œè³‡è¨Šï¼‰
    if state.get("cache_hit"):
        state["answer"] = state.get("answer", "")
        state["evidence_summary"] = state.get("evidence_summary", "")
        return state

    kb_text = "\n".join(
        [f"[{i+1}] {x.get('title','')}\nURL: {x.get('url','')}\næ‘˜è¦: {x.get('snippet','')}"
         for i, x in enumerate(kb[:8])]
    )

    # å¯é¸ï¼šè‹¥å•é¡Œå¸¶ URL ä¸” ENABLE_VLM_READ=Trueï¼Œå¯è®€ç¶²é è£œ kbï¼ˆé€™æ®µä¸å½±éŸ¿ä¸»æµç¨‹ï¼‰
    if ENABLE_VLM_READ:
        # ç²—ç•¥æŠ“ URL
        import re
        m = re.search(r"https?://\S+", q)
        if m:
            url = m.group(0)
            print(f"ğŸ” VLM è®€ç¶²é ï¼š{url}")
            try:
                text = vlm_read_website(url)
                state["kb"].append({"title": "VLM ç¶²é æ‘˜éŒ„", "url": url, "snippet": text[:800]})
            except Exception as e:
                print(f"VLM è®€å–å¤±æ•—ï¼š{e}")

    prompt = f"""
ä½ æ˜¯ä¸€å€‹ã€Œè‡ªå‹•æŸ¥è­‰ AIã€ã€‚è«‹ç”¨ç›®å‰è³‡æ–™å›ç­”å•é¡Œï¼Œæ ¼å¼å›ºå®šå¦‚ä¸‹ï¼š

1) çµè«–ï¼šä¸€å¥è©±å›ç­”ï¼ˆé¿å…èª‡å¤§ï¼Œè‹¥ä¸ç¢ºå®šè¦èªªä¸ç¢ºå®šï¼‰
2) è­‰æ“šæ‘˜è¦ï¼šåˆ—å‡º 2~4 é»ï¼Œä¾†æºåªç”¨ knowledge_baseï¼ˆå¯ç”¨ã€Œå¾æŸç¶²ç«™/æŸå–®ä½ã€æè¿°ï¼Œä¸å¿…çœŸçš„å¼•ç”¨æ ¼å¼ï¼‰
3) é™åˆ¶/ä¸ç¢ºå®šæ€§ï¼šèªªæ˜è³‡æ–™ç¼ºå£æˆ–å¯èƒ½èª¤å·®ï¼Œçµ¦ä¸‹ä¸€æ­¥å»ºè­°

ã€å•é¡Œã€‘
{q}

ã€knowledge_baseã€‘
{kb_text}
"""
    ans = llm.invoke([HumanMessage(content=prompt)]).content.strip()
    state["answer"] = ans

    # è­‰æ“šæ‘˜è¦å¦å¤–å­˜ä¸€ä»½ï¼ˆçµ¦ README æˆ– debug ç”¨ï¼‰
    ev_prompt = f"""
è«‹æŠŠ knowledge_base å…§å®¹æ¿ƒç¸®æˆ 3~6 è¡Œã€Œè­‰æ“šæ‘˜è¦ã€ï¼Œæ¯è¡ŒåŒ…å«ï¼š
- ä¾†æº/ç¶²ç«™åï¼ˆæˆ–æ¨™é¡Œï¼‰
- å¤§è‡´èªªæ˜å®ƒæ”¯æŒäº†ä»€éº¼
åªè¼¸å‡ºæ‘˜è¦ï¼ˆä¸è¦å¤šé¤˜æ–‡å­—ï¼‰ã€‚

knowledge_baseï¼š
{kb_text}
"""
    evidence_summary = llm.invoke([HumanMessage(content=ev_prompt)]).content.strip()
    state["evidence_summary"] = evidence_summary

    # å¯«å…¥ cache
    key = get_clean_key(q)
    save_cache(key, {
        "question": q,
        "answer": ans,
        "evidence_summary": evidence_summary,
        "kb": kb,
        "ts": time.time()
    })
    print("âœ… å·²å¯«å…¥å¿«å–")

    return state


# =========================
# 5) Routersï¼ˆæ±ºç­–é‚è¼¯ï¼‰
# =========================
def after_cache_router(state: State) -> Literal["final_answer", "planner"]:
    # hit -> ç›´æ¥ final_answerï¼ˆè£¡é¢æœƒç›´æ¥å›å‚³ cache çš„ç­”æ¡ˆï¼‰
    if state.get("cache_hit"):
        return "final_answer"
    return "planner"


def master_router(state: State) -> Literal["final_answer", "query_gen"]:
    """
    planner ä¹‹å¾Œçš„è·¯ç”±ï¼ˆç…§ä½ è¦çš„ 3 è·¯é‚è¼¯ï¼‰ï¼š
    - è‹¥ kb è¶³å¤  â†’ final_answer
    - å¦å‰‡ â†’ query_genï¼ˆå†å» search_toolï¼‰
    """
    # loop é˜²å‘†ï¼šplanner_node å·²ç¶“æœ‰ä¿è­·ï¼Œé€™è£¡è£œä¸€å±¤
    if state.get("loop", 0) >= 3:
        return "final_answer"

    # kb ç©ºå°±å¿…é ˆ query_gen
    if not state.get("kb"):
        return "query_gen"

    # å†åšä¸€æ¬¡è¼•é‡åˆ¤æ–·ï¼šè®“ planner_node å·²ç¶“åˆ¤æ–·éçš„çµæœç”Ÿæ•ˆ
    # é€™è£¡æœ€ä¿å®ˆï¼šåªè¦ kb æœ‰è³‡æ–™ï¼Œä½†ä»ä¸è¶³ï¼Œå°± query_gen
    # æˆ‘å€‘ç”¨åŒä¸€å€‹åˆ¤æ–· prompt å†è·‘ä¸€æ¬¡æœƒæµªè²» tokenï¼Œæ‰€ä»¥æ”¹ç”¨ç°¡åŒ–è¦å‰‡ï¼š
    # - kb >= 3 ç­†ï¼šå…ˆå˜—è©¦ final_answerï¼ˆå¤šæ•¸å¯å›ç­”ï¼‰
    # - å¦å‰‡ï¼šå…ˆ query_gen å†è£œè³‡æ–™
    if len(state["kb"]) >= 3:
        return "final_answer"
    return "query_gen"


# =========================
# 6) çµ„ Graphï¼ˆä¿è­‰ä¸æœƒå¤šæ¥ search->final/endï¼‰
# =========================
workflow = StateGraph(State)

workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("final_answer", final_answer_node)

workflow.set_entry_point("check_cache")

# check_cache -> (hit) final_answer -> END
# check_cache -> (miss) planner
workflow.add_conditional_edges(
    "check_cache",
    after_cache_router,
    {
        "final_answer": "final_answer",
        "planner": "planner",
    }
)

# planner -> (enough) final_answer
# planner -> (not enough) query_gen
workflow.add_conditional_edges(
    "planner",
    master_router,
    {
        "final_answer": "final_answer",
        "query_gen": "query_gen",
    }
)

# query_gen -> search_tool -> plannerï¼ˆåªå› plannerï¼‰
workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")

# final_answer -> END
workflow.add_edge("final_answer", END)

app = workflow.compile()


# =========================
# 7) Debug / æª¢æŸ¥å·¥å…·
# =========================
def print_edges():
    g = app.get_graph()
    print("\nEdges:")
    for e in sorted(g.edges, key=lambda x: (x.source, x.target)):
        print(" ", e)

def print_mermaid():
    # é€™æœƒè¼¸å‡º Mermaid åŸå§‹ç¢¼ï¼ˆæ²’æœ‰åœ–æ˜¯æ­£å¸¸çš„ï¼Œå› ç‚º terminal ä¸æœƒæ¸²æŸ“ï¼‰
    try:
        g = app.get_graph()
        print(g.draw_mermaid())
    except Exception as e:
        print("draw_mermaid å¤±æ•—ï¼š", e)


# =========================
# 8) CLI
# =========================
if __name__ == "__main__":
    # ä½ è¦ç¢ºèªç·šè·¯å°ä¸å° â†’ å…ˆå° edges æœ€æº–
    print_edges()
    print(app.get_graph().draw_ascii())
    # æƒ³è¦ Mermaid åŸå§‹ç¢¼ä¹Ÿå¯ä»¥å°ï¼ˆè²¼åˆ°æ”¯æ´ Mermaid çš„åœ°æ–¹æ‰æœƒè®Šåœ–ï¼‰
    # print_mermaid()

    while True:
        user_input = input("\nè¼¸å…¥å•é¡Œï¼ˆq é›¢é–‹ï¼‰ï¼š").strip()
        if user_input.lower() in ["q", "quit", "exit"]:
            break

        inputs: State = {
            "question": user_input,
            "answer": "",
            "cache_hit": False,
            "kb": [],
            "search_query": "",
            "loop": 0,
            "evidence_summary": "",
        }

        start = time.time()
        result = app.invoke(inputs)
        end = time.time()

        print("\n========== æœ€çµ‚ç­”æ¡ˆ ==========")
        print(result["answer"])
        print("\n( Cache Hit:", result.get("cache_hit", False), ")")
        print("( è€—æ™‚: %.3f ç§’ )" % (end - start))
